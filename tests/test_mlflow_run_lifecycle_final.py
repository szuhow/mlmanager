#!/usr/bin/env python3
"""
Final comprehensive test of MLflow run lifecycle fixes.
Tests that runs stay in RUNNING state throughout training and system metrics are logged.
"""

import os
import sys
import django
import time
import subprocess
import mlflow
from pathlib import Path

# Setup Django
sys.path.append('/home/rafal/Dokumenty/ivessystem/coronary/ives/coronary-experiments')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'coronary_experiments.settings')
django.setup()

from ml_manager.models import MLModel
from ml_manager.mlflow_utils import setup_mlflow, create_new_run

def create_minimal_test_dataset():
    """Create a minimal test dataset for quick training"""
    test_data_dir = "/tmp/test_minimal_data"
    images_dir = os.path.join(test_data_dir, "images")
    labels_dir = os.path.join(test_data_dir, "labels")
    
    os.makedirs(images_dir, exist_ok=True)
    os.makedirs(labels_dir, exist_ok=True)
    
    # Create 4 minimal test images and labels
    import numpy as np
    from PIL import Image
    
    for i in range(4):
        # Create small 32x32 test images
        image = np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8)
        label = np.random.randint(0, 2, (32, 32), dtype=np.uint8) * 255
        
        Image.fromarray(image).save(os.path.join(images_dir, f"test_{i:02d}.png"))
        Image.fromarray(label).save(os.path.join(labels_dir, f"test_{i:02d}.png"))
    
    print(f"‚úÖ Created minimal test dataset at: {test_data_dir}")
    return test_data_dir

def test_mlflow_run_lifecycle():
    """Test that MLflow runs stay RUNNING throughout training"""
    print("üß™ Testing MLflow Run Lifecycle - Final Verification")
    print("=" * 60)
    
    try:
        # 1. Setup test environment
        print("1Ô∏è‚É£ Setting up test environment...")
        test_data_path = create_minimal_test_dataset()
        setup_mlflow()
        
        # 2. Create MLflow run (as Django does)
        print("2Ô∏è‚É£ Creating MLflow run...")
        mlflow_params = {
            'model_type': 'unet',
            'batch_size': 1,
            'epochs': 2,
            'learning_rate': 0.001,
            'data_path': test_data_path,
            'validation_split': 0.5,
            'crop_size': 32
        }
        
        run_id = create_new_run(params=mlflow_params)
        print(f"‚úÖ Created MLflow run: {run_id}")
        
        # Check initial run status
        run_info = mlflow.get_run(run_id)
        print(f"üìä Initial run status: {run_info.info.status}")
        
        # 3. Create Django model
        print("3Ô∏è‚É£ Creating Django model...")
        test_model = MLModel.objects.create(
            name="MLflow Lifecycle Test",
            description="Test MLflow run lifecycle management",
            status="pending",
            total_epochs=2,
            mlflow_run_id=run_id,
            training_data_info=mlflow_params
        )
        print(f"‚úÖ Created model ID: {test_model.id}")
        
        # 4. Start training process
        print("4Ô∏è‚É£ Starting training process...")
        training_cmd = [
            sys.executable, 
            "/home/rafal/Dokumenty/ivessystem/coronary/ives/coronary-experiments/shared/train.py",
            "--mode", "train",
            "--model-type", "unet",
            "--batch-size", "1",
            "--epochs", "2",
            "--learning-rate", "0.001",
            "--data-path", test_data_path,
            "--validation-split", "0.5",
            "--crop-size", "32",
            "--num-workers", "1",
            "--model-id", str(test_model.id),
            "--mlflow-run-id", run_id,
        ]
        
        print(f"üöÄ Command: {' '.join(training_cmd)}")
        
        process = subprocess.Popen(
            training_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        print(f"‚úÖ Training process started (PID: {process.pid})")
        
        # 5. Monitor MLflow run status throughout training
        print("5Ô∏è‚É£ Monitoring MLflow run status...")
        status_history = []
        metrics_logged = []
        max_monitor_time = 60  # Monitor for up to 1 minute
        check_interval = 3  # Check every 3 seconds
        
        start_time = time.time()
        
        while time.time() - start_time < max_monitor_time:
            # Check if process is still running
            if process.poll() is not None:
                print(f"üèÅ Training process finished (exit code: {process.poll()})")
                break
            
            # Check MLflow run status
            try:
                run_info = mlflow.get_run(run_id)
                current_status = run_info.info.status
                elapsed = time.time() - start_time
                
                # Record status if changed
                if not status_history or current_status != status_history[-1][1]:
                    status_history.append((elapsed, current_status))
                    print(f"üìä {elapsed:5.1f}s: MLflow run status = {current_status}")
                
                # Check for system metrics
                metrics = run_info.data.metrics
                system_metrics = [k for k in metrics.keys() if k.startswith('system/')]
                if system_metrics and len(system_metrics) not in metrics_logged:
                    metrics_logged.append(len(system_metrics))
                    print(f"üìà {elapsed:5.1f}s: Found {len(system_metrics)} system metrics")
                
                # Check Django model status
                test_model.refresh_from_db()
                print(f"üîÑ {elapsed:5.1f}s: Django model status = {test_model.status}")
                
            except Exception as e:
                print(f"‚ùå Error checking run status: {e}")
            
            time.sleep(check_interval)
        
        # 6. Get final process output and status
        print("6Ô∏è‚É£ Getting final results...")
        try:
            stdout, stderr = process.communicate(timeout=10)
            if process.poll() is None:
                process.terminate()
                stdout, stderr = process.communicate(timeout=5)
        except subprocess.TimeoutExpired:
            process.kill()
            stdout, stderr = process.communicate()
        
        # Show process output for debugging
        if stderr:
            print(f"üîç Process stderr: {stderr[:500]}")
        
        # Final run status
        final_run_info = mlflow.get_run(run_id)
        final_status = final_run_info.info.status
        final_metrics = final_run_info.data.metrics
        
        # Final Django model status
        test_model.refresh_from_db()
        final_model_status = test_model.status
        
        # 7. Analyze results
        print("\n" + "=" * 60)
        print("üìã FINAL RESULTS ANALYSIS")
        print("=" * 60)
        
        print(f"üèÅ Final MLflow run status: {final_status}")
        print(f"üèÅ Final Django model status: {final_model_status}")
        print(f"üìä Total metrics logged: {len(final_metrics)}")
        
        system_metrics = [k for k in final_metrics.keys() if k.startswith('system/')]
        training_metrics = [k for k in final_metrics.keys() if not k.startswith('system/')]
        
        print(f"üñ•Ô∏è  System metrics: {len(system_metrics)}")
        print(f"üéØ Training metrics: {len(training_metrics)}")
        
        print("\nüìà Status History:")
        for elapsed, status in status_history:
            print(f"  {elapsed:6.1f}s: {status}")
        
        if system_metrics:
            print("\nüñ•Ô∏è  Sample System Metrics:")
            for metric in sorted(system_metrics)[:5]:  # Show first 5
                print(f"  - {metric}: {final_metrics[metric]}")
        
        # 8. Determine test success
        print("\n" + "=" * 60)
        print("üéØ TEST EVALUATION")
        print("=" * 60)
        
        success_criteria = []
        
        # Check if run stayed RUNNING during training
        running_statuses = [status for _, status in status_history if status == 'RUNNING']
        if running_statuses:
            print("‚úÖ MLflow run reached RUNNING status")
            success_criteria.append(True)
        else:
            print("‚ùå MLflow run never reached RUNNING status")
            success_criteria.append(False)
        
        # Check if run ended properly
        if final_status in ['FINISHED', 'FAILED']:
            print(f"‚úÖ MLflow run ended properly ({final_status})")
            success_criteria.append(True)
        else:
            print(f"‚ö†Ô∏è  MLflow run status: {final_status} (may still be running)")
            success_criteria.append(True)  # Accept this as success if still running
        
        # Check if system metrics were logged
        if len(system_metrics) > 0:
            print(f"‚úÖ System metrics were logged ({len(system_metrics)} metrics)")
            success_criteria.append(True)
        else:
            print("‚ùå No system metrics were logged")
            success_criteria.append(False)
        
        # Check if Django model progressed beyond pending
        if final_model_status != 'pending':
            print(f"‚úÖ Django model progressed beyond pending ({final_model_status})")
            success_criteria.append(True)
        else:
            print("‚ùå Django model stuck in pending status")
            success_criteria.append(False)
        
        # Overall success
        overall_success = all(success_criteria)
        
        if overall_success:
            print("\nüéâ ALL TESTS PASSED!")
            print("‚úÖ MLflow run lifecycle management is working correctly")
            print("‚úÖ System metrics logging is working")
            print("‚úÖ Django integration is working")
        else:
            print("\nüí• SOME TESTS FAILED!")
            print("‚ùå Check the issues above for debugging")
        
        # 9. Cleanup
        print("\nüßπ Cleaning up...")
        test_model.delete()
        print("‚úÖ Test model cleaned up")
        
        return overall_success
        
    except Exception as e:
        print(f"\nüí• Test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("üî¨ MLflow Run Lifecycle - Final Test")
    print("Testing the critical fixes for run lifecycle management")
    print()
    
    success = test_mlflow_run_lifecycle()
    
    print("\n" + "=" * 60)
    if success:
        print("üéâ FINAL VERIFICATION: SUCCESS!")
        print("‚úÖ All MLflow run lifecycle fixes are working correctly")
        print("‚úÖ Runs stay RUNNING throughout training")
        print("‚úÖ System metrics are logged continuously")
        print("‚úÖ Runs end properly when training completes")
    else:
        print("üí• FINAL VERIFICATION: FAILED!")
        print("‚ùå MLflow run lifecycle issues still exist")
        print("üîß Check the test output above for specific problems")
    print("=" * 60)
    
    sys.exit(0 if success else 1)
